defaults:
    - hydra/job_logging: colorlog
    - hydra/hydra_logging: colorlog
hydra:
    run:
      # constant if you want to resume the training
      dir: ./outputs/gcp_base_exp3 # ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} #./outputs/exp1 # 
training:
    resume: true
    # reproduce
    random_seed: 123
    num_gpus_per_node: 8
    # Mixed Precision
    fp16: true
    fp16_opt_level: "O1" # O0 to disable fp16
    # Optimization
    learning_rate: 1e-3
    gradient_accumulation_steps: 32 
    max_gradient_norm: -1.0 # disabled when negative. used for clip_grad_norm
    optimizer: "AdamW"
    scheduler: "WarmupLinear" # WarmupConstant, WarmupLinear, WarmupCosine, WarmupCosineWithHardRestarts
    warmup_steps: 10000 # disabled when negative 
    weight_decay: 0.01
    batch_size: 8
    total_num_epochs: -1
    total_num_update_steps: 1e6 # disabled when total_num_epochs > 0
    validation_steps_interval: -1 # set to infinite to disable validation
saving:
    # when saving.steps_interval and saving.seconds_interval
    # are both negative, save the models for every epoch
    steps_interval: -1
    seconds_interval: 1800 # disabled when steps_interval > 1
    # checkpointer
    num_checkpoints_to_keep: 16
    keep_checkpoint_every_num_seconds: 43200
logging:
    log_dir: "logs"
    color: true
    level: "INFO"   
    steps_interval: -1 # disabled when negative
    seconds_interval: 10 # disabled when `log_steps_interval` is set
model:
    max_position_embeddings: 520
    vocab_size: 50265
    num_attention_heads: 12
    num_hidden_layers: 12
    hidden_size: 768
    intermediate_size: 3072
    hidden_dropout_prob: 0.0
    attention_probs_dropout_prob: 0.0
    gradient_checkpointing: False
    hidden_act: swish
    layer_norm_eps: 1e-5
    embedding_std: 0.1
    pad_token_id: 1
    sep_token_id: 2
data:
    data_dir: /data
    total_num_sectors: 128
    max_seq_len: 512
    num_cls_tokens: 1
    pad_token_id: 1
    sep_token_id: 2
