defaults:
    - hydra/job_logging: colorlog
    - hydra/hydra_logging: colorlog
hydra:
    run:
      # constant if you want to resume the training
      dir: ./outputs/exp1 # ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} #./outputs/exp1 # 
training:
    resume: true
    # reproduce
    random_seed: 123
    num_gpus_per_node: 2
    # Mixed Precision
    fp16: true
    fp16_opt_level: "O2" # O0 to disable fp16
    # Optimization
    learning_rate: 2e-4
    gradient_accumulation_steps: 8
    max_gradient_norm: -1.0 # disabled when negative. used for clip_grad_norm
    optimizer: "AdamW"
    scheduler: "WarmupLinear" # WarmupConstant, WarmupLinear, WarmupCosine, WarmupCosineWithHardRestarts
    warmup_steps: 100 # disabled when negative 
    weight_decay: 0.01
    batch_size: 2
    total_num_epochs: -1
    total_num_steps: 10e6 # disabled when total_num_epochs > 0
    validation_steps_interval: -1 # set to infinite to disable validation
saving:
    # when saving.steps_interval and saving.seconds_interval
    # are both negative, save the models for every epoch
    steps_interval: -1
    seconds_interval: 1800 # disabled when steps_interval > 1
    # checkpointer
    num_checkpoints_to_keep: 5
    keep_checkpoint_every_num_seconds: 3600
logging:
    log_dir: "logs"
    color: true
    level: "INFO"   
    steps_interval: -1 # disabled when negative
    seconds_interval: 2 # disabled when `log_steps_interval` is set
model:
    max_position_embeddings: 520
    vocab_size: 50265
    persistent_mem_size: 64
    num_attention_heads: 16
    num_hidden_layers: 24
    hidden_size: 1024
    intermediate_size: 4096
    hidden_dropout_prob: 0.05
    attention_probs_dropout_prob: 0.05
    gradient_checkpointing: True
    hidden_act: swish
    layer_norm_eps: 1e-5
    embedding_std: 0.1
    pad_token_id: 1
    sep_token_id: 2
task:
    max_seq_len: 510
    num_cls_tokens: 5